{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:27.879529Z",
     "start_time": "2024-11-29T02:32:27.873435Z"
    }
   },
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime \n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import f1_score\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "1918ebf27650f4f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:31.963055Z",
     "start_time": "2024-11-29T02:32:27.901604Z"
    }
   },
   "source": [
    "raw_data = pd.read_csv('../Data/santander-customer-transaction-prediction/train.csv')\n",
    "raw_data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "7da8d11456109b64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:31.974043Z",
     "start_time": "2024-11-29T02:32:31.965992Z"
    }
   },
   "source": [
    "raw_data.dtypes.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    200\n",
       "object       1\n",
       "int64        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "6f5fdb10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:31.983144Z",
     "start_time": "2024-11-29T02:32:31.976009Z"
    }
   },
   "source": [
    "raw_data['target'].dtype"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "b64349bd81a7bb5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:33.760370Z",
     "start_time": "2024-11-29T02:32:31.985106Z"
    }
   },
   "source": [
    "raw_data.describe()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "              target          var_0          var_1          var_2  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        0.100490      10.679914      -1.627622      10.715192   \n",
       "std         0.300653       3.040051       4.050044       2.640894   \n",
       "min         0.000000       0.408400     -15.043400       2.117100   \n",
       "25%         0.000000       8.453850      -4.740025       8.722475   \n",
       "50%         0.000000      10.524750      -1.608050      10.580000   \n",
       "75%         0.000000      12.758200       1.358625      12.516700   \n",
       "max         1.000000      20.315000      10.376800      19.353000   \n",
       "\n",
       "               var_3          var_4          var_5          var_6  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        6.796529      11.078333      -5.065317       5.408949   \n",
       "std         2.043319       1.623150       7.863267       0.866607   \n",
       "min        -0.040200       5.074800     -32.562600       2.347300   \n",
       "25%         5.254075       9.883175     -11.200350       4.767700   \n",
       "50%         6.825000      11.108250      -4.833150       5.385100   \n",
       "75%         8.324100      12.261125       0.924800       6.003000   \n",
       "max        13.188300      16.671400      17.251600       8.447700   \n",
       "\n",
       "               var_7          var_8  ...        var_190        var_191  \\\n",
       "count  200000.000000  200000.000000  ...  200000.000000  200000.000000   \n",
       "mean       16.545850       0.284162  ...       3.234440       7.438408   \n",
       "std         3.418076       3.332634  ...       4.559922       3.023272   \n",
       "min         5.349700     -10.505500  ...     -14.093300      -2.691700   \n",
       "25%        13.943800      -2.317800  ...      -0.058825       5.157400   \n",
       "50%        16.456800       0.393700  ...       3.203600       7.347750   \n",
       "75%        19.102900       2.937900  ...       6.406200       9.512525   \n",
       "max        27.691800      10.151300  ...      18.440900      16.716500   \n",
       "\n",
       "             var_192        var_193        var_194        var_195  \\\n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000   \n",
       "mean        1.927839       3.331774      17.993784      -0.142088   \n",
       "std         1.478423       3.992030       3.135162       1.429372   \n",
       "min        -3.814500     -11.783400       8.694400      -5.261000   \n",
       "25%         0.889775       0.584600      15.629800      -1.170700   \n",
       "50%         1.901300       3.396350      17.957950      -0.172700   \n",
       "75%         2.949500       6.205800      20.396525       0.829600   \n",
       "max         8.402400      18.281800      27.928800       4.272900   \n",
       "\n",
       "             var_196        var_197        var_198        var_199  \n",
       "count  200000.000000  200000.000000  200000.000000  200000.000000  \n",
       "mean        2.303335       8.908158      15.870720      -3.326537  \n",
       "std         5.454369       0.921625       3.010945      10.438015  \n",
       "min       -14.209600       5.960600       6.299300     -38.852800  \n",
       "25%        -1.946925       8.252800      13.829700     -11.208475  \n",
       "50%         2.408900       8.888200      15.934050      -2.819550  \n",
       "75%         6.556725       9.593300      18.064725       4.836800  \n",
       "max        18.321500      12.000400      26.079100      28.500700  \n",
       "\n",
       "[8 rows x 201 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "      <td>200000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.100490</td>\n",
       "      <td>10.679914</td>\n",
       "      <td>-1.627622</td>\n",
       "      <td>10.715192</td>\n",
       "      <td>6.796529</td>\n",
       "      <td>11.078333</td>\n",
       "      <td>-5.065317</td>\n",
       "      <td>5.408949</td>\n",
       "      <td>16.545850</td>\n",
       "      <td>0.284162</td>\n",
       "      <td>...</td>\n",
       "      <td>3.234440</td>\n",
       "      <td>7.438408</td>\n",
       "      <td>1.927839</td>\n",
       "      <td>3.331774</td>\n",
       "      <td>17.993784</td>\n",
       "      <td>-0.142088</td>\n",
       "      <td>2.303335</td>\n",
       "      <td>8.908158</td>\n",
       "      <td>15.870720</td>\n",
       "      <td>-3.326537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.300653</td>\n",
       "      <td>3.040051</td>\n",
       "      <td>4.050044</td>\n",
       "      <td>2.640894</td>\n",
       "      <td>2.043319</td>\n",
       "      <td>1.623150</td>\n",
       "      <td>7.863267</td>\n",
       "      <td>0.866607</td>\n",
       "      <td>3.418076</td>\n",
       "      <td>3.332634</td>\n",
       "      <td>...</td>\n",
       "      <td>4.559922</td>\n",
       "      <td>3.023272</td>\n",
       "      <td>1.478423</td>\n",
       "      <td>3.992030</td>\n",
       "      <td>3.135162</td>\n",
       "      <td>1.429372</td>\n",
       "      <td>5.454369</td>\n",
       "      <td>0.921625</td>\n",
       "      <td>3.010945</td>\n",
       "      <td>10.438015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408400</td>\n",
       "      <td>-15.043400</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>-0.040200</td>\n",
       "      <td>5.074800</td>\n",
       "      <td>-32.562600</td>\n",
       "      <td>2.347300</td>\n",
       "      <td>5.349700</td>\n",
       "      <td>-10.505500</td>\n",
       "      <td>...</td>\n",
       "      <td>-14.093300</td>\n",
       "      <td>-2.691700</td>\n",
       "      <td>-3.814500</td>\n",
       "      <td>-11.783400</td>\n",
       "      <td>8.694400</td>\n",
       "      <td>-5.261000</td>\n",
       "      <td>-14.209600</td>\n",
       "      <td>5.960600</td>\n",
       "      <td>6.299300</td>\n",
       "      <td>-38.852800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.453850</td>\n",
       "      <td>-4.740025</td>\n",
       "      <td>8.722475</td>\n",
       "      <td>5.254075</td>\n",
       "      <td>9.883175</td>\n",
       "      <td>-11.200350</td>\n",
       "      <td>4.767700</td>\n",
       "      <td>13.943800</td>\n",
       "      <td>-2.317800</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058825</td>\n",
       "      <td>5.157400</td>\n",
       "      <td>0.889775</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>15.629800</td>\n",
       "      <td>-1.170700</td>\n",
       "      <td>-1.946925</td>\n",
       "      <td>8.252800</td>\n",
       "      <td>13.829700</td>\n",
       "      <td>-11.208475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.524750</td>\n",
       "      <td>-1.608050</td>\n",
       "      <td>10.580000</td>\n",
       "      <td>6.825000</td>\n",
       "      <td>11.108250</td>\n",
       "      <td>-4.833150</td>\n",
       "      <td>5.385100</td>\n",
       "      <td>16.456800</td>\n",
       "      <td>0.393700</td>\n",
       "      <td>...</td>\n",
       "      <td>3.203600</td>\n",
       "      <td>7.347750</td>\n",
       "      <td>1.901300</td>\n",
       "      <td>3.396350</td>\n",
       "      <td>17.957950</td>\n",
       "      <td>-0.172700</td>\n",
       "      <td>2.408900</td>\n",
       "      <td>8.888200</td>\n",
       "      <td>15.934050</td>\n",
       "      <td>-2.819550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.758200</td>\n",
       "      <td>1.358625</td>\n",
       "      <td>12.516700</td>\n",
       "      <td>8.324100</td>\n",
       "      <td>12.261125</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.102900</td>\n",
       "      <td>2.937900</td>\n",
       "      <td>...</td>\n",
       "      <td>6.406200</td>\n",
       "      <td>9.512525</td>\n",
       "      <td>2.949500</td>\n",
       "      <td>6.205800</td>\n",
       "      <td>20.396525</td>\n",
       "      <td>0.829600</td>\n",
       "      <td>6.556725</td>\n",
       "      <td>9.593300</td>\n",
       "      <td>18.064725</td>\n",
       "      <td>4.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.315000</td>\n",
       "      <td>10.376800</td>\n",
       "      <td>19.353000</td>\n",
       "      <td>13.188300</td>\n",
       "      <td>16.671400</td>\n",
       "      <td>17.251600</td>\n",
       "      <td>8.447700</td>\n",
       "      <td>27.691800</td>\n",
       "      <td>10.151300</td>\n",
       "      <td>...</td>\n",
       "      <td>18.440900</td>\n",
       "      <td>16.716500</td>\n",
       "      <td>8.402400</td>\n",
       "      <td>18.281800</td>\n",
       "      <td>27.928800</td>\n",
       "      <td>4.272900</td>\n",
       "      <td>18.321500</td>\n",
       "      <td>12.000400</td>\n",
       "      <td>26.079100</td>\n",
       "      <td>28.500700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 201 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "c784edabc7994ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:33.781578Z",
     "start_time": "2024-11-29T02:32:33.764285Z"
    }
   },
   "source": [
    "raw_data.info()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "6fcf0febffcb8335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:33.854407Z",
     "start_time": "2024-11-29T02:32:33.783539Z"
    }
   },
   "source": [
    "raw_data.drop('ID_code', axis=1, inplace=True)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "de73973b0447e7fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:34.068594Z",
     "start_time": "2024-11-29T02:32:33.856368Z"
    }
   },
   "source": [
    "x = raw_data.drop(['target'], axis=1)\n",
    "y = raw_data['target']"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "d006e55f129b9a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:35.468115Z",
     "start_time": "2024-11-29T02:32:34.073477Z"
    }
   },
   "source": [
    "x_train, x_test , y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "c010e47f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:35.479013Z",
     "start_time": "2024-11-29T02:32:35.469638Z"
    }
   },
   "source": [
    "y_train.value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    144001\n",
       "1     15999\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "fb1d3f0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:35.484780Z",
     "start_time": "2024-11-29T02:32:35.480926Z"
    }
   },
   "source": [
    "# sc = StandardScaler()\n",
    "# x_train = sc.fit_transform(x_train)\n",
    "# x_test = sc.transform(x_test)\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "8d1c2222",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:35.492315Z",
     "start_time": "2024-11-29T02:32:35.486742Z"
    }
   },
   "source": [
    "# pca = PCA(random_state=42)\n",
    "# x_train_pca = pca.fit_transform(x_train)\n",
    "# x_test_pca = pca.transform(x_test)\n",
    "# var_ratio_sum = np.cumsum(pca.explained_variance_ratio_)\n",
    "# threshold = np.arange(0.0, 1.01, 0.01, )\n",
    "# dimension = lambda threshold: np.argmax(var_ratio_sum>=threshold) + 1\n",
    "# \n",
    "# \n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot([dimension(x) for x in threshold],threshold, label = 'PCA')\n",
    "# plt.title('Explained variance as function of the numbers of dimensions', pad=15)\n",
    "# plt.ylabel('Explained Variance')\n",
    "# plt.xlabel('Dimension')\n",
    "# plt.legend(loc=\"lower right\", fontsize=8)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "70affa15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:32:47.079487Z",
     "start_time": "2024-11-29T02:32:35.494274Z"
    }
   },
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#logistic regression with pca\n",
    "\n",
    "logisticRegression = LogisticRegression(random_state=42, solver='lbfgs',\n",
    "    n_jobs=-1,class_weight='balanced')\n",
    "logisticRegression.fit(x_train, y_train)\n",
    "\n",
    "log_y_pre = logisticRegression.predict(x_test)\n",
    "log_y_proba = logisticRegression.predict_proba(x_test)[:, 1]\n",
    "log_confusion_matrix = confusion_matrix(y_test, log_y_pre)\n",
    "log_accuracy = accuracy_score(y_test, log_y_pre)\n",
    "log_f1_score = f1_score(y_test, log_y_pre, average='weighted')\n",
    "log_pre = precision_score(y_test, log_y_pre, average='weighted')\n",
    "log_recall = recall_score(y_test, log_y_pre, average='weighted')\n",
    "log_roc_auc = roc_auc_score(y_test, log_y_proba)\n",
    "\n",
    "print(\"Accuracy:\", log_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", log_confusion_matrix)\n",
    "print(\"F1 Score:\", log_f1_score)\n",
    "print(\"Precision Score:\", log_pre)\n",
    "print(\"Recall Score:\", log_recall)\n",
    "print(\"ROC AUC Score:\", log_roc_auc)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, log_y_pre))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.772425\n",
      "Confusion Matrix:\n",
      " [[27755  8146]\n",
      " [  957  3142]]\n",
      "F1 Score: 0.8129276768513948\n",
      "Precision Score: 0.8961333658641334\n",
      "Recall Score: 0.772425\n",
      "ROC AUC Score: 0.8508488745503062\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.77      0.86     35901\n",
      "           1       0.28      0.77      0.41      4099\n",
      "\n",
      "    accuracy                           0.77     40000\n",
      "   macro avg       0.62      0.77      0.63     40000\n",
      "weighted avg       0.90      0.77      0.81     40000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "4f9c9773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:33:08.025351Z",
     "start_time": "2024-11-29T02:32:47.081453Z"
    }
   },
   "source": [
    "#SDGClassifier with PCA\n",
    "sgd_clf = SGDClassifier( loss='log_loss',max_iter=200, tol =0.001, random_state=42)\n",
    "sgd_clf.fit(x_train, y_train)\n",
    "sgd_clf_y_pred = sgd_clf.predict(x_test)\n",
    "\n",
    "svm_confusion_matrix = confusion_matrix(y_test, sgd_clf_y_pred)\n",
    "svm_accuracy = accuracy_score(y_test, sgd_clf_y_pred)\n",
    "svm_precision = precision_score(y_test, sgd_clf_y_pred)\n",
    "svm_recall = recall_score(y_test, sgd_clf_y_pred)\n",
    "svm_f1 = f1_score(y_test, sgd_clf_y_pred)\n",
    "svm_y_proba = sgd_clf.predict_proba(x_test)[:,1]\n",
    "svm_roc_auc = roc_auc_score(y_test, svm_y_proba)\n",
    "\n",
    "print(\"SVM confusion matrix: \\n\", svm_confusion_matrix)\n",
    "print(\"SVM accuracy: \", svm_accuracy)\n",
    "print(\"SVM precision: \", svm_precision)\n",
    "print(\"SVM recall: \", svm_recall)\n",
    "print(\"SVM f1: \", svm_f1)\n",
    "print(\"SVM ROC-AUC: \", svm_roc_auc)\n",
    "print(classification_report(y_test, sgd_clf_y_pred))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM confusion matrix: \n",
      " [[35811    90]\n",
      " [ 3767   332]]\n",
      "SVM accuracy:  0.903575\n",
      "SVM precision:  0.7867298578199052\n",
      "SVM recall:  0.0809953647231032\n",
      "SVM f1:  0.1468701614687016\n",
      "SVM ROC-AUC:  0.8280468083195281\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     35901\n",
      "           1       0.79      0.08      0.15      4099\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.85      0.54      0.55     40000\n",
      "weighted avg       0.89      0.90      0.87     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "fd526171",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:35:19.320269Z",
     "start_time": "2024-11-29T02:33:08.028290Z"
    }
   },
   "source": [
    "#RandomForest with PCA\n",
    "randomForestClassifier = RandomForestClassifier(random_state=42, n_jobs = -1)\n",
    "randomForestClassifier.fit(x_train, y_train)\n",
    "randomForestClassifier_y_pred = randomForestClassifier.predict(x_test)\n",
    "\n",
    "rf_confusion_matrix = confusion_matrix(y_test, randomForestClassifier_y_pred)\n",
    "rf_accuracy = accuracy_score(y_test, randomForestClassifier_y_pred)\n",
    "rf_precision = precision_score(y_test, randomForestClassifier_y_pred)\n",
    "rf_recall = recall_score(y_test, randomForestClassifier_y_pred)\n",
    "rf_f1 = f1_score(y_test, randomForestClassifier_y_pred)\n",
    "log_y_proba = randomForestClassifier.predict_proba(x_test)[:,1]\n",
    "roc_auc = roc_auc_score(y_test, log_y_proba)\n",
    "\n",
    "print(\"rf confusion matrix: \\n\", rf_confusion_matrix)\n",
    "print(\"rf accuracy: \", rf_accuracy)\n",
    "print(\"rf precision: \", rf_precision)\n",
    "print(\"rf recall: \", rf_recall)\n",
    "print(\"rf f1: \", rf_f1)\n",
    "print(\"rf ROC-AUC: \", roc_auc)\n",
    "print(classification_report(y_test, randomForestClassifier_y_pred))\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf confusion matrix: \n",
      " [[35901     0]\n",
      " [ 4099     0]]\n",
      "rf accuracy:  0.897525\n",
      "rf precision:  0.0\n",
      "rf recall:  0.0\n",
      "rf f1:  0.0\n",
      "rf ROC-AUC:  0.816749894445229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95     35901\n",
      "           1       0.00      0.00      0.00      4099\n",
      "\n",
      "    accuracy                           0.90     40000\n",
      "   macro avg       0.45      0.50      0.47     40000\n",
      "weighted avg       0.81      0.90      0.85     40000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "a1512e3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:35:43.388710Z",
     "start_time": "2024-11-29T02:35:19.321254Z"
    }
   },
   "source": [
    "xgb_model = xgb.XGBClassifier(\n",
    "    booster='gbtree',                   \n",
    "    objective='binary:logistic',                              \n",
    "    learning_rate=0.5,\n",
    "    n_estimators=500,                   \n",
    "    subsample=1,                        \n",
    "    colsample_bytree=1.0,                                                               \n",
    "    gamma = 0,\n",
    "    scale_pos_weight=len(y_train) / (2 * np.bincount(y_train)[1])          \n",
    "    ) \n",
    "xgb_model.fit(x_train, y_train)\n",
    "xgbrf_y_pred = xgb_model.predict(x_test)\n",
    "\n",
    "xgbrf_confusion_matrix = confusion_matrix(y_test, xgbrf_y_pred)\n",
    "xgbrf_accuracy = accuracy_score(y_test, xgbrf_y_pred)\n",
    "xgbrf_precision = precision_score(y_test, xgbrf_y_pred)\n",
    "xgbrf_recall = recall_score(y_test, xgbrf_y_pred)\n",
    "xgbrf_f1 = f1_score(y_test, xgbrf_y_pred)\n",
    "xgbrf_y_proba = randomForestClassifier.predict_proba(x_test)[:,1]\n",
    "xgbrf_roc_auc = roc_auc_score(y_test, xgbrf_y_proba)\n",
    "\n",
    "print(\"rf confusion matrix: \\n\", xgbrf_confusion_matrix)\n",
    "print(\"rf accuracy: \", xgbrf_accuracy)\n",
    "print(\"rf precision: \", xgbrf_precision)\n",
    "print(\"rf recall: \", xgbrf_recall)\n",
    "print(\"rf f1: \", xgbrf_f1)\n",
    "print(\"rf ROC-AUC: \", xgbrf_roc_auc)\n",
    "print(classification_report(y_test, xgbrf_y_pred))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf confusion matrix: \n",
      " [[34885  1016]\n",
      " [ 2629  1470]]\n",
      "rf accuracy:  0.908875\n",
      "rf precision:  0.5913113435237329\n",
      "rf recall:  0.358624054647475\n",
      "rf f1:  0.44646924829157175\n",
      "rf ROC-AUC:  0.816749894445229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95     35901\n",
      "           1       0.59      0.36      0.45      4099\n",
      "\n",
      "    accuracy                           0.91     40000\n",
      "   macro avg       0.76      0.67      0.70     40000\n",
      "weighted avg       0.90      0.91      0.90     40000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "631383dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:35:43.398465Z",
     "start_time": "2024-11-29T02:35:43.389695Z"
    }
   },
   "source": [
    "importances = xgb_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 81 (0.016411)\n",
      "2. feature 139 (0.014082)\n",
      "3. feature 12 (0.012588)\n",
      "4. feature 53 (0.012289)\n",
      "5. feature 110 (0.011642)\n",
      "6. feature 174 (0.011142)\n",
      "7. feature 26 (0.010453)\n",
      "8. feature 109 (0.010292)\n",
      "9. feature 6 (0.010291)\n",
      "10. feature 146 (0.010255)\n",
      "11. feature 166 (0.010119)\n",
      "12. feature 22 (0.009984)\n",
      "13. feature 76 (0.009660)\n",
      "14. feature 165 (0.009477)\n",
      "15. feature 99 (0.009450)\n",
      "16. feature 0 (0.009363)\n",
      "17. feature 21 (0.009078)\n",
      "18. feature 133 (0.009071)\n",
      "19. feature 80 (0.009053)\n",
      "20. feature 78 (0.008893)\n",
      "21. feature 198 (0.008677)\n",
      "22. feature 13 (0.008454)\n",
      "23. feature 2 (0.008407)\n",
      "24. feature 108 (0.008320)\n",
      "25. feature 170 (0.008275)\n",
      "26. feature 34 (0.008263)\n",
      "27. feature 44 (0.008246)\n",
      "28. feature 33 (0.008094)\n",
      "29. feature 148 (0.007983)\n",
      "30. feature 190 (0.007982)\n",
      "31. feature 179 (0.007773)\n",
      "32. feature 164 (0.007753)\n",
      "33. feature 184 (0.007709)\n",
      "34. feature 169 (0.007663)\n",
      "35. feature 191 (0.007593)\n",
      "36. feature 94 (0.007507)\n",
      "37. feature 177 (0.007416)\n",
      "38. feature 92 (0.007344)\n",
      "39. feature 121 (0.007264)\n",
      "40. feature 149 (0.007151)\n",
      "41. feature 1 (0.007032)\n",
      "42. feature 40 (0.006998)\n",
      "43. feature 115 (0.006805)\n",
      "44. feature 154 (0.006746)\n",
      "45. feature 122 (0.006724)\n",
      "46. feature 173 (0.006720)\n",
      "47. feature 9 (0.006630)\n",
      "48. feature 192 (0.006599)\n",
      "49. feature 123 (0.006333)\n",
      "50. feature 118 (0.006258)\n",
      "51. feature 18 (0.006238)\n",
      "52. feature 91 (0.006129)\n",
      "53. feature 197 (0.006112)\n",
      "54. feature 56 (0.005933)\n",
      "55. feature 107 (0.005899)\n",
      "56. feature 127 (0.005875)\n",
      "57. feature 36 (0.005838)\n",
      "58. feature 67 (0.005803)\n",
      "59. feature 157 (0.005797)\n",
      "60. feature 188 (0.005737)\n",
      "61. feature 155 (0.005644)\n",
      "62. feature 89 (0.005557)\n",
      "63. feature 95 (0.005552)\n",
      "64. feature 75 (0.005538)\n",
      "65. feature 86 (0.005530)\n",
      "66. feature 172 (0.005498)\n",
      "67. feature 119 (0.005464)\n",
      "68. feature 147 (0.005417)\n",
      "69. feature 5 (0.005335)\n",
      "70. feature 87 (0.005202)\n",
      "71. feature 93 (0.005199)\n",
      "72. feature 151 (0.005160)\n",
      "73. feature 141 (0.005157)\n",
      "74. feature 35 (0.005140)\n",
      "75. feature 180 (0.005110)\n",
      "76. feature 163 (0.005094)\n",
      "77. feature 130 (0.005080)\n",
      "78. feature 51 (0.004969)\n",
      "79. feature 167 (0.004962)\n",
      "80. feature 49 (0.004954)\n",
      "81. feature 106 (0.004870)\n",
      "82. feature 145 (0.004764)\n",
      "83. feature 52 (0.004756)\n",
      "84. feature 48 (0.004740)\n",
      "85. feature 131 (0.004709)\n",
      "86. feature 24 (0.004709)\n",
      "87. feature 137 (0.004687)\n",
      "88. feature 150 (0.004639)\n",
      "89. feature 114 (0.004570)\n",
      "90. feature 71 (0.004557)\n",
      "91. feature 162 (0.004552)\n",
      "92. feature 43 (0.004549)\n",
      "93. feature 32 (0.004453)\n",
      "94. feature 28 (0.004395)\n",
      "95. feature 194 (0.004393)\n",
      "96. feature 23 (0.004392)\n",
      "97. feature 70 (0.004388)\n",
      "98. feature 104 (0.004260)\n",
      "99. feature 85 (0.004235)\n",
      "100. feature 195 (0.004174)\n",
      "101. feature 135 (0.004171)\n",
      "102. feature 105 (0.004122)\n",
      "103. feature 90 (0.004042)\n",
      "104. feature 175 (0.004042)\n",
      "105. feature 199 (0.004037)\n",
      "106. feature 186 (0.003994)\n",
      "107. feature 58 (0.003892)\n",
      "108. feature 111 (0.003832)\n",
      "109. feature 144 (0.003827)\n",
      "110. feature 128 (0.003822)\n",
      "111. feature 88 (0.003815)\n",
      "112. feature 125 (0.003796)\n",
      "113. feature 97 (0.003776)\n",
      "114. feature 132 (0.003761)\n",
      "115. feature 193 (0.003719)\n",
      "116. feature 196 (0.003701)\n",
      "117. feature 74 (0.003689)\n",
      "118. feature 112 (0.003652)\n",
      "119. feature 77 (0.003643)\n",
      "120. feature 8 (0.003635)\n",
      "121. feature 82 (0.003628)\n",
      "122. feature 31 (0.003614)\n",
      "123. feature 11 (0.003612)\n",
      "124. feature 116 (0.003568)\n",
      "125. feature 134 (0.003499)\n",
      "126. feature 55 (0.003497)\n",
      "127. feature 19 (0.003494)\n",
      "128. feature 54 (0.003486)\n",
      "129. feature 83 (0.003476)\n",
      "130. feature 66 (0.003401)\n",
      "131. feature 181 (0.003385)\n",
      "132. feature 142 (0.003381)\n",
      "133. feature 168 (0.003370)\n",
      "134. feature 178 (0.003332)\n",
      "135. feature 45 (0.003318)\n",
      "136. feature 25 (0.003288)\n",
      "137. feature 156 (0.003266)\n",
      "138. feature 68 (0.003201)\n",
      "139. feature 185 (0.003179)\n",
      "140. feature 65 (0.003158)\n",
      "141. feature 102 (0.003153)\n",
      "142. feature 64 (0.003142)\n",
      "143. feature 138 (0.003119)\n",
      "144. feature 187 (0.003082)\n",
      "145. feature 171 (0.003077)\n",
      "146. feature 4 (0.003064)\n",
      "147. feature 59 (0.003060)\n",
      "148. feature 38 (0.003055)\n",
      "149. feature 15 (0.003036)\n",
      "150. feature 20 (0.003008)\n",
      "151. feature 113 (0.002998)\n",
      "152. feature 62 (0.002954)\n",
      "153. feature 159 (0.002939)\n",
      "154. feature 29 (0.002931)\n",
      "155. feature 143 (0.002930)\n",
      "156. feature 30 (0.002928)\n",
      "157. feature 16 (0.002913)\n",
      "158. feature 152 (0.002910)\n",
      "159. feature 57 (0.002875)\n",
      "160. feature 72 (0.002824)\n",
      "161. feature 176 (0.002764)\n",
      "162. feature 17 (0.002758)\n",
      "163. feature 14 (0.002757)\n",
      "164. feature 50 (0.002756)\n",
      "165. feature 161 (0.002753)\n",
      "166. feature 73 (0.002751)\n",
      "167. feature 27 (0.002729)\n",
      "168. feature 3 (0.002718)\n",
      "169. feature 60 (0.002717)\n",
      "170. feature 140 (0.002705)\n",
      "171. feature 182 (0.002698)\n",
      "172. feature 120 (0.002692)\n",
      "173. feature 47 (0.002639)\n",
      "174. feature 39 (0.002590)\n",
      "175. feature 153 (0.002583)\n",
      "176. feature 98 (0.002578)\n",
      "177. feature 61 (0.002577)\n",
      "178. feature 189 (0.002520)\n",
      "179. feature 63 (0.002507)\n",
      "180. feature 158 (0.002503)\n",
      "181. feature 183 (0.002493)\n",
      "182. feature 129 (0.002483)\n",
      "183. feature 96 (0.002475)\n",
      "184. feature 41 (0.002459)\n",
      "185. feature 84 (0.002440)\n",
      "186. feature 7 (0.002428)\n",
      "187. feature 37 (0.002424)\n",
      "188. feature 69 (0.002422)\n",
      "189. feature 10 (0.002407)\n",
      "190. feature 101 (0.002401)\n",
      "191. feature 42 (0.002400)\n",
      "192. feature 46 (0.002388)\n",
      "193. feature 103 (0.002382)\n",
      "194. feature 100 (0.002368)\n",
      "195. feature 124 (0.002343)\n",
      "196. feature 79 (0.002332)\n",
      "197. feature 126 (0.002308)\n",
      "198. feature 117 (0.002239)\n",
      "199. feature 160 (0.002236)\n",
      "200. feature 136 (0.002138)\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "c16d8620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:35:43.435077Z",
     "start_time": "2024-11-29T02:35:43.399449Z"
    }
   },
   "source": [
    "importances = randomForestClassifier.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(x_train.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 81 (0.012269)\n",
      "2. feature 139 (0.010011)\n",
      "3. feature 12 (0.009793)\n",
      "4. feature 53 (0.008859)\n",
      "5. feature 110 (0.008704)\n",
      "6. feature 146 (0.008474)\n",
      "7. feature 174 (0.008450)\n",
      "8. feature 166 (0.008317)\n",
      "9. feature 6 (0.007994)\n",
      "10. feature 22 (0.007952)\n",
      "11. feature 26 (0.007931)\n",
      "12. feature 109 (0.007856)\n",
      "13. feature 80 (0.007584)\n",
      "14. feature 76 (0.007378)\n",
      "15. feature 99 (0.007213)\n",
      "16. feature 78 (0.007034)\n",
      "17. feature 198 (0.007030)\n",
      "18. feature 133 (0.006993)\n",
      "19. feature 165 (0.006919)\n",
      "20. feature 0 (0.006709)\n",
      "21. feature 13 (0.006686)\n",
      "22. feature 2 (0.006640)\n",
      "23. feature 190 (0.006630)\n",
      "24. feature 44 (0.006518)\n",
      "25. feature 179 (0.006507)\n",
      "26. feature 170 (0.006471)\n",
      "27. feature 40 (0.006427)\n",
      "28. feature 148 (0.006400)\n",
      "29. feature 34 (0.006387)\n",
      "30. feature 21 (0.006357)\n",
      "31. feature 164 (0.006244)\n",
      "32. feature 94 (0.006244)\n",
      "33. feature 1 (0.006127)\n",
      "34. feature 33 (0.006098)\n",
      "35. feature 191 (0.006063)\n",
      "36. feature 108 (0.005989)\n",
      "37. feature 154 (0.005988)\n",
      "38. feature 18 (0.005916)\n",
      "39. feature 177 (0.005884)\n",
      "40. feature 184 (0.005801)\n",
      "41. feature 169 (0.005733)\n",
      "42. feature 9 (0.005703)\n",
      "43. feature 123 (0.005681)\n",
      "44. feature 115 (0.005678)\n",
      "45. feature 92 (0.005678)\n",
      "46. feature 91 (0.005570)\n",
      "47. feature 121 (0.005556)\n",
      "48. feature 95 (0.005537)\n",
      "49. feature 86 (0.005517)\n",
      "50. feature 155 (0.005504)\n",
      "51. feature 188 (0.005467)\n",
      "52. feature 75 (0.005425)\n",
      "53. feature 122 (0.005341)\n",
      "54. feature 173 (0.005324)\n",
      "55. feature 147 (0.005318)\n",
      "56. feature 56 (0.005275)\n",
      "57. feature 127 (0.005269)\n",
      "58. feature 67 (0.005252)\n",
      "59. feature 180 (0.005234)\n",
      "60. feature 93 (0.005175)\n",
      "61. feature 119 (0.005144)\n",
      "62. feature 89 (0.005128)\n",
      "63. feature 151 (0.005122)\n",
      "64. feature 157 (0.005115)\n",
      "65. feature 172 (0.005109)\n",
      "66. feature 118 (0.005069)\n",
      "67. feature 5 (0.005057)\n",
      "68. feature 36 (0.005055)\n",
      "69. feature 130 (0.005051)\n",
      "70. feature 131 (0.005050)\n",
      "71. feature 51 (0.005037)\n",
      "72. feature 197 (0.005033)\n",
      "73. feature 149 (0.005021)\n",
      "74. feature 141 (0.004999)\n",
      "75. feature 32 (0.004942)\n",
      "76. feature 107 (0.004928)\n",
      "77. feature 125 (0.004914)\n",
      "78. feature 49 (0.004878)\n",
      "79. feature 145 (0.004835)\n",
      "80. feature 163 (0.004829)\n",
      "81. feature 167 (0.004805)\n",
      "82. feature 35 (0.004786)\n",
      "83. feature 132 (0.004741)\n",
      "84. feature 87 (0.004686)\n",
      "85. feature 195 (0.004672)\n",
      "86. feature 43 (0.004672)\n",
      "87. feature 106 (0.004656)\n",
      "88. feature 194 (0.004650)\n",
      "89. feature 71 (0.004595)\n",
      "90. feature 162 (0.004591)\n",
      "91. feature 135 (0.004585)\n",
      "92. feature 114 (0.004564)\n",
      "93. feature 111 (0.004537)\n",
      "94. feature 128 (0.004536)\n",
      "95. feature 175 (0.004521)\n",
      "96. feature 90 (0.004515)\n",
      "97. feature 137 (0.004514)\n",
      "98. feature 150 (0.004493)\n",
      "99. feature 196 (0.004469)\n",
      "100. feature 79 (0.004469)\n",
      "101. feature 105 (0.004463)\n",
      "102. feature 192 (0.004452)\n",
      "103. feature 55 (0.004446)\n",
      "104. feature 82 (0.004435)\n",
      "105. feature 85 (0.004426)\n",
      "106. feature 102 (0.004420)\n",
      "107. feature 24 (0.004418)\n",
      "108. feature 83 (0.004412)\n",
      "109. feature 88 (0.004411)\n",
      "110. feature 48 (0.004408)\n",
      "111. feature 134 (0.004395)\n",
      "112. feature 52 (0.004389)\n",
      "113. feature 144 (0.004380)\n",
      "114. feature 15 (0.004376)\n",
      "115. feature 116 (0.004366)\n",
      "116. feature 23 (0.004365)\n",
      "117. feature 69 (0.004349)\n",
      "118. feature 152 (0.004296)\n",
      "119. feature 3 (0.004295)\n",
      "120. feature 193 (0.004284)\n",
      "121. feature 97 (0.004270)\n",
      "122. feature 156 (0.004268)\n",
      "123. feature 70 (0.004260)\n",
      "124. feature 186 (0.004243)\n",
      "125. feature 58 (0.004240)\n",
      "126. feature 64 (0.004240)\n",
      "127. feature 50 (0.004229)\n",
      "128. feature 37 (0.004221)\n",
      "129. feature 143 (0.004218)\n",
      "130. feature 171 (0.004210)\n",
      "131. feature 68 (0.004194)\n",
      "132. feature 168 (0.004184)\n",
      "133. feature 142 (0.004180)\n",
      "134. feature 112 (0.004177)\n",
      "135. feature 46 (0.004175)\n",
      "136. feature 178 (0.004168)\n",
      "137. feature 20 (0.004167)\n",
      "138. feature 66 (0.004163)\n",
      "139. feature 28 (0.004149)\n",
      "140. feature 113 (0.004144)\n",
      "141. feature 153 (0.004126)\n",
      "142. feature 181 (0.004120)\n",
      "143. feature 199 (0.004117)\n",
      "144. feature 25 (0.004114)\n",
      "145. feature 59 (0.004113)\n",
      "146. feature 19 (0.004113)\n",
      "147. feature 31 (0.004111)\n",
      "148. feature 63 (0.004110)\n",
      "149. feature 104 (0.004103)\n",
      "150. feature 16 (0.004103)\n",
      "151. feature 138 (0.004100)\n",
      "152. feature 60 (0.004087)\n",
      "153. feature 11 (0.004081)\n",
      "154. feature 187 (0.004077)\n",
      "155. feature 7 (0.004072)\n",
      "156. feature 39 (0.004071)\n",
      "157. feature 8 (0.004064)\n",
      "158. feature 41 (0.004057)\n",
      "159. feature 54 (0.004049)\n",
      "160. feature 4 (0.004043)\n",
      "161. feature 72 (0.004042)\n",
      "162. feature 182 (0.004028)\n",
      "163. feature 161 (0.004006)\n",
      "164. feature 101 (0.004006)\n",
      "165. feature 159 (0.003985)\n",
      "166. feature 27 (0.003982)\n",
      "167. feature 57 (0.003966)\n",
      "168. feature 38 (0.003960)\n",
      "169. feature 65 (0.003960)\n",
      "170. feature 84 (0.003959)\n",
      "171. feature 129 (0.003956)\n",
      "172. feature 126 (0.003953)\n",
      "173. feature 14 (0.003944)\n",
      "174. feature 77 (0.003942)\n",
      "175. feature 62 (0.003932)\n",
      "176. feature 140 (0.003928)\n",
      "177. feature 42 (0.003927)\n",
      "178. feature 74 (0.003924)\n",
      "179. feature 183 (0.003918)\n",
      "180. feature 189 (0.003916)\n",
      "181. feature 124 (0.003915)\n",
      "182. feature 73 (0.003910)\n",
      "183. feature 30 (0.003909)\n",
      "184. feature 96 (0.003904)\n",
      "185. feature 61 (0.003897)\n",
      "186. feature 98 (0.003892)\n",
      "187. feature 45 (0.003891)\n",
      "188. feature 120 (0.003870)\n",
      "189. feature 176 (0.003868)\n",
      "190. feature 160 (0.003861)\n",
      "191. feature 17 (0.003847)\n",
      "192. feature 47 (0.003813)\n",
      "193. feature 117 (0.003813)\n",
      "194. feature 29 (0.003796)\n",
      "195. feature 185 (0.003795)\n",
      "196. feature 10 (0.003780)\n",
      "197. feature 100 (0.003756)\n",
      "198. feature 136 (0.003707)\n",
      "199. feature 158 (0.003701)\n",
      "200. feature 103 (0.003672)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "0fc2291c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T02:43:42.344392Z",
     "start_time": "2024-11-29T02:35:43.436065Z"
    }
   },
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[('lr', LogisticRegression(random_state=41, n_jobs = -1)),\n",
    "                ('rf', RandomForestClassifier(random_state=41, n_jobs = -1)),\n",
    "                ('sgd',SGDClassifier(random_state=41, n_jobs = -1)),\n",
    "                ('xgb', XGBClassifier(booster='gbtree',random_state=41))], final_estimator=RandomForestClassifier(random_state=43, n_jobs = -1),cv=3)\n",
    "stacking_clf.fit(x_train, y_train)\n",
    "stacking_clf_y_ped = stacking_clf.predict(x_test)\n",
    "\n",
    "stacking_clf_confusion_matrix = confusion_matrix(y_test, stacking_clf_y_ped)\n",
    "stacking_clf_accuracy = accuracy_score(y_test, stacking_clf_y_ped)\n",
    "stacking_clf_precision = precision_score(y_test, stacking_clf_y_ped)\n",
    "stacking_clf_recall = recall_score(y_test, stacking_clf_y_ped)\n",
    "stacking_clf_f1 = f1_score(y_test, stacking_clf_y_ped)\n",
    "stacking_clf_y_proba = randomForestClassifier.predict_proba(x_test)[:,1]\n",
    "stacking_clf_roc_auc = roc_auc_score(y_test, xgbrf_y_proba)\n",
    "\n",
    "print(\"rf confusion matrix: \\n\", rf_confusion_matrix)\n",
    "print(\"rf accuracy: \", stacking_clf_accuracy)\n",
    "print(\"rf precision: \", stacking_clf_precision)\n",
    "print(\"rf recall: \", stacking_clf_recall)\n",
    "print(\"rf f1: \", stacking_clf_f1)\n",
    "print(\"rf ROC-AUC: \", stacking_clf_roc_auc)\n",
    "print(classification_report(y_test, stacking_clf_y_ped))\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf confusion matrix: \n",
      " [[35901     0]\n",
      " [ 4099     0]]\n",
      "rf accuracy:  0.91505\n",
      "rf precision:  0.6576698155645524\n",
      "rf recall:  0.35667235911197853\n",
      "rf f1:  0.4625118633343879\n",
      "rf ROC-AUC:  0.816749894445229\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95     35901\n",
      "           1       0.66      0.36      0.46      4099\n",
      "\n",
      "    accuracy                           0.92     40000\n",
      "   macro avg       0.79      0.67      0.71     40000\n",
      "weighted avg       0.90      0.92      0.90     40000\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
